![Cover Image](Assets/cover.png)

<h1 align="center">Smart Pot : An intelligent Plant Care Assistant </h1>

"here comes the badges"

Smart Pot is an intelligent plant care assistant that uses AI and IoT to help users monitor and interact with their tomato plants. The system combines a YOLOv5-based detection model, ESP32 with environmental sensors, and a mobile app with a built-in chatbot interface.

## ğŸ“š Table of Contents

- [ğŸš€ Features](#-features)
- [ğŸ§  Tech Stack](#-tech-stack)
- [ğŸ“± Screenshots](#-screenshots)
- [ğŸ› ï¸ Installation](#ï¸-installation)
  - [ğŸ”Œ Backend (Flask Server)](#-backend-flask-server)
  - [ğŸ”§ ESP32 (Sensor Upload)](#-esp32-sensor-upload)
  - [ğŸ“ Raspberry Pi (YOLOv5 Deployment)](#-raspberry-pi-yolov5-deployment)
  - [ğŸ“² Mobile App (React Native)](#-mobile-app-react-native)
- [ğŸ“Š Project Architecture](#-project-architecture)
- [âœ… Testing](#-testing)
  - [LLM Chatbot](#llm-chatbot)
  - [YOLOv5 Model](#yolov5-model)
  - [Watering System](#watering-system)
- [ğŸ“ Folder Structure](#-folder-structure)
- [ğŸ“ˆ Future Improvements](#-future-improvements)
- [ğŸ‘¨â€ğŸ’» Authors](#-authors)
- [ğŸ“œ License](#-license)

## ğŸš€ Features

- ğŸ” **Plant Detection**: Identifies the plant using a YOLOv5 object detection model.
- ğŸŒ¡ï¸ **Sensor Monitoring**: Collects real-time light and moisture levels using an ESP32 microcontroller and uploads data to Firebase.
- ğŸ¤– **Chatbot Interface**: A natural language chatbot (LLM-based) to interact with the plant and provide care suggestions.
- ğŸ“± **React Native App**: Mobile front-end for users to view sensor data and chat with the Smart Pot.

---

## ğŸ§  Tech Stack

<table border="1">
  <tr>
    <th>Component</th>
    <th>Technology Used</th>
  </tr>
  <tr>
    <td>Object Detection</td>
    <td>YOLOv5 (PyTorch)</td>
  </tr>
  <tr>
    <td>Microcontroller</td>
    <td>ESP32 with Light & Moisture Sensors</td>
  </tr>
  <tr>
    <td>Cloud Database</td>
    <td>Firebase Realtime Database</td>
  </tr>
  <tr>
    <td>Backend Server</td>
    <td>Python (Fast API)</td>
  </tr>
  <tr>
    <td>Chatbot Model</td>
    <td>LLM via Distilled Bert Model</td>
  </tr>
  <tr>
    <td>Mobile App</td>
    <td>React Native (Expo)</td>
  </tr>
</table>

---

## ğŸ“± Screenshots

"screenshots to be posted here"


## ğŸ› ï¸ Installation

### ğŸ”Œ Backend (Flask Server)
### ğŸ”§ ESP32 (Sensor Upload)
### ğŸ“ Raspberry Pi (YOLOv5 Deployment)
### ğŸ“² Mobile App (React Native)
```
cd mobile-app
npm install
npx expo start
```

## ğŸ“Š Project Architecture
![Architecture Image](Assets/architecture.png)

## âœ… Testing
### LLM ChatBot
<table border="1">
  <tr>
    <th>Test Case</th>
    <th>Input</th>
    <th>Expected Output</th>
    <th>Status</th>
  </tr>
  <tr>
    <td>Query plant name</td>
    <td>"What's my plant?"</td>
    <td>"Tomato"</td>
    <td>âœ…</td>
  </tr>
  <tr>
    <td>Ask moisture</td>
    <td>"Is my plant thirsty?"</td>
    <td>"Yes, soil is dry"</td>
    <td>âœ…</td>
  </tr>
</table>

### Yolov5 Model
### Watering System

## ğŸ“ Folder Structure

SmartPot/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ esp32/
â”‚   â””â”€â”€ esp32_sensor.ino
â”œâ”€â”€ raspberry-pi/
â”‚   â”œâ”€â”€ detect.py
â”‚   â””â”€â”€ yolov5_model/
â”œâ”€â”€ mobile-app/
â”‚   â”œâ”€â”€ App.js
â”‚   â””â”€â”€ components/
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ testing.md
â”œâ”€â”€ README.md


## Section 1 setting up the LLM-Model.

option one download pretrained model.
option two train it yourself

 python 3.11 is tested + discsalimer for untested versions
    
    
    create virtual env name chatbotenv
    and activate the virtual env
    navigate to chat-bot-server folder
    install requiremrnts .txt
    then 

option 1:


    run train_llm.py

option 2:

    download the pre trained model by running the script download.py

now the model is downloaded and and ready to use

    start the fast api server that accepts the queries from and forwards to it the llm model which then returns the output.
  // word it better later
  using this command
  uvicorn main:app --host 0.0.0.0 --reload

    now llm model is running in local host.


## Section 2 Mobile Application 
    